# Conclusions

```{epigraph}
[... Man] has no time to be anything but a machine. How can he remember well his ignorance--which his growth requires--who has so often to use his knowledge?

-- Henry David Thoreau, _Walden; or, life in the woods_
```

The use of HPC solutions allowed calibrating GEOtop within a reasonable time and with acceptable results, notwithstanding the large parameters space. The code developed, which is published and freely available on GitHub, also shows how libraries and tools used within the machine learning community could be useful and easily adapted to Earth-system and environmental models calibration. Finally, a simple performance model for the computation has been discussed.

Some examples of further research topics are:

* empirical studies on the choice of optimization algorithms and hyperparameters,
* determination of the optimal population size,  
* use of local optimization algorithms for refinement of the solutions,
* interpretation and validation of the solutions.


## The Beautiful Ineffectiveness of Computer Science

The main problem of present scientific research is the adoption of the ideas and methods from free market. Research institutes must profit from the work of researchers and profit is measured in publications, prestige and funding. The phenomenon is not new, but the degree to which it permeates academia is unedited. Also, it is tightly linked to the sociopolitical and economic system in which we live, and it is probably irreversible. This problem not only intoxicate the academic environment and the life of people which work in academia, but also affects negatively the quality of research. It creates positive feedback loops and inflates some research topics while stagnating others. We live in a time of a profound, yet silent, crisis in science.

Of course, technology and applied science, which have an immediate return, are more likely to be funded and hence get more attention from researchers. In some cases, the situation is exacerbated by lack of scientific contents and rigour. Prominent examples of speculative bubbles are HPC and some trending topics in machine learning, as artificial neural networks. Private investments in the loop worse the situation.

Publications regarding these tools are mostly about technological advances, of varying interest and value. However, the consensus is considering them a scientific breakthrough. Certainly, they are not completely useless, but neither are the answer to every possible question. On the contrary, the ubiquitous application of artificial neural networks to modeling and inference, as surrogate human understanding, is the utmost failure of reductionism. The use of mathematical language, scientific method, and reductionism fueled the most spectacular achievements in our understanding of nature: all of them are in danger.

Some problems in modern science requires HPC, although fewer than promised by exascale evangelists. Nonetheless, is it HPC an interesting scientific topic per se? I think that it is. Understanding the performance of large distributed systems, running thousands of coordinated processes, is an interesting subject. However, it needs a paradigm shift: it should be investigated for the sake of it, out of pure curiosity. The accent should be on comprehension, not on making things and doing stuff. Paradoxically, it is from gratuitous knowledge that the greatest technological advances come out, in the long run.

A similar situation exists in computer science (which according to Hal Abelson is not a science and is not about computers). Its cultural relevance and brief history rich of beautiful ideas are not recognized, and it is poorly taught. As long as we see it as a screwdriver, or a hammer, we are doomed to poorly reinvent the wheel. Only when we will see its beautiful ineffectiveness we will make real scientific progress.
