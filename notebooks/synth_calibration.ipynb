{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Defaults\n",
    "\n",
    "# System settings\n",
    "systmpfs = '/tmp'\n",
    "address = 'auto'\n",
    "inputs_path = 'inputs/Matsch_B2/run'\n",
    "geotop_path = '../geotop/build/geotop'\n",
    "variables_path = 'inputs/Matsch_B2/variables.csv'\n",
    "\n",
    "# Optimizer settings\n",
    "num_workers = 2\n",
    "budget = 16\n",
    "algorithm = 'OnePlusOne'\n",
    "timeout = 120\n",
    "monitor_interval = 10\n",
    "scale = 'D'\n",
    "startdate = '01/01/2011 00:00'\n",
    "targets = ['soil_moisture_content_50', 'sensible_heat_flux_in_air']\n",
    "weights = [1.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from subprocess import CalledProcessError, TimeoutExpired\n",
    "from tempfile import TemporaryDirectory, NamedTemporaryFile\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import uniform\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nevergrad as ng\n",
    "import hiplot as hip\n",
    "from SALib.analyze import delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as joinpath\n",
    "from threading import Thread\n",
    "from time import sleep\n",
    "from collections.abc import Mapping\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import psutil\n",
    "import geotopy as gtp\n",
    "\n",
    "def date_parser(x):\n",
    "    return datetime.strptime(x, '%d/%m/%Y %H:%M')\n",
    "\n",
    "class GEOtopRun(gtp.GEOtop):\n",
    "\n",
    "    def preprocess(self, working_dir, *args, **kwargs):\n",
    "        \n",
    "        settings = {**self, **kwargs}\n",
    "        \n",
    "        inpts_src_path = joinpath(self.inputs_dir, 'geotop.inpts')\n",
    "        inpts_dest_path = joinpath(working_dir, 'geotop.inpts')\n",
    "        \n",
    "        with open(inpts_src_path, 'r') as inpts_src, open(inpts_dest_path, 'w') as inpts_dest:\n",
    "            inpts_dest.write(f\"! GEOtop input file written by GEOtoPy {datetime.now().strftime('%x %X')}\\n\")\n",
    "            while line := inpts_src.readline():\n",
    "                if gtp._comment_re.match(line):\n",
    "                    inpts_dest.write(line)\n",
    "                else:\n",
    "                    try:\n",
    "                        key, value = gtp.read_setting(line)\n",
    "                        \n",
    "                        if key in settings and value != settings[key]:\n",
    "                            inpts_dest.write(f\"! GEOtoPy: {key} overwritten, was {value}\\n\")\n",
    "                            line = gtp.print_setting(key, settings[key])\n",
    "                        else:\n",
    "                            line = gtp.print_setting(key, value)\n",
    "                        \n",
    "                        inpts_dest.write(line)\n",
    "                        del settings[key]\n",
    "                    \n",
    "                    except ValueError as err:\n",
    "                        inpts_dest.write(f\"! GEOtoPy: {err}\\n\")\n",
    "                        inpts_dest.write(line)\n",
    "            \n",
    "            if settings:\n",
    "                inpts_dest.write(\"\\n! Settings added by GEOtoPy\\n\")\n",
    "                for key, value in settings.items():\n",
    "                    try:\n",
    "                        line = gtp.print_setting(key, value)\n",
    "                        inpts_dest.write(line)\n",
    "                    except ValueError as err:\n",
    "                        inpts_dest.write(f\"! GEOtoPy: {err}\\n\")\n",
    "                        inpts_dest.write(f\"{key} = {value}\\n\")\n",
    "                            \n",
    "        \n",
    "    def postprocess(self, working_dir):\n",
    "        \n",
    "        liq_path = joinpath(working_dir, 'theta_liq.txt')\n",
    "        liq = pd.read_csv(liq_path, \n",
    "                          na_values=['-9999'],\n",
    "                          usecols=[0, 6, 7], \n",
    "                          skiprows=1,\n",
    "                          header=0, \n",
    "                          names=['datetime', 'soil_moisture_content_50', 'soil_moisture_content_200'],\n",
    "                          parse_dates=[0], \n",
    "                          date_parser=date_parser,\n",
    "                          index_col=0,\n",
    "                          low_memory=False)\n",
    "        \n",
    "        ice_path = joinpath(working_dir, 'theta_ice.txt')\n",
    "        ice = pd.read_csv(ice_path, \n",
    "                          na_values=['-9999'], \n",
    "                          usecols=[0, 6, 7], \n",
    "                          skiprows=1,\n",
    "                          header=0, \n",
    "                          names=['datetime', 'soil_moisture_content_50', 'soil_moisture_content_200'],\n",
    "                          parse_dates=[0], \n",
    "                          date_parser=date_parser,\n",
    "                          index_col=0,\n",
    "                          low_memory=False)\n",
    "        \n",
    "        point_path = joinpath(working_dir, 'point.txt')\n",
    "        point = pd.read_csv(point_path, \n",
    "                          na_values=['-9999'],\n",
    "                          parse_dates=[0], \n",
    "                          date_parser=date_parser,\n",
    "                          index_col=0,\n",
    "                          low_memory=False)\n",
    "        point.index.rename('datetime', inplace=True)\n",
    "        \n",
    "        sim = pd.DataFrame(index=point.index)\n",
    "        \n",
    "        sim['rainfall_amount'] = point['Prain_over_canopy[mm]'] + point['Psnow_over_canopy[mm]']\n",
    "        \n",
    "        sim['wind_speed'] = point['Wind_speed[m/s]']\n",
    "        \n",
    "        sim['relative_humidity'] = point['Relative_Humidity[-]']\n",
    "        \n",
    "        sim['air_temperature'] = point['Tair[C]']\n",
    "        \n",
    "        sim['surface_downwelling_shortwave_flux'] = point['SWin[W/m2]']\n",
    "        \n",
    "        sim['soil_moisture_content_50'] = ice['soil_moisture_content_50'] + liq['soil_moisture_content_50']\n",
    "        \n",
    "        sim['soil_moisture_content_200'] = ice['soil_moisture_content_200'] + liq['soil_moisture_content_200']\n",
    "        \n",
    "        sim['latent_heat_flux_in_air'] = \\\n",
    "            point['Canopy_fraction[-]'] * (point['LEg_veg[W/m2]'] + point['LEv[W/m2]']) + \\\n",
    "            (1 - point['Canopy_fraction[-]']) * point['LEg_unveg[W/m2]']\n",
    "        \n",
    "        sim['sensible_heat_flux_in_air'] = \\\n",
    "            point['Canopy_fraction[-]'] * (point['Hg_veg[W/m2]'] + point['Hv[W/m2]']) + \\\n",
    "            (1 - point['Canopy_fraction[-]']) * point['Hg_unveg[W/m2]']\n",
    "        \n",
    "        return sim\n",
    "\n",
    "class observations(Mapping):\n",
    "    \n",
    "    def __init__(self, source, scale='D', start=None, end=None):\n",
    "        \n",
    "        self.scale = scale\n",
    "        \n",
    "        if isinstance(source, pd.DataFrame):\n",
    "            obs = source\n",
    "        else:\n",
    "            obs = pd.read_csv(source, \n",
    "                              na_values=['-9999', '-99.99'],\n",
    "                              parse_dates=[0], \n",
    "                              date_parser=date_parser,\n",
    "                              index_col=0)\n",
    "        \n",
    "        obs.index.rename('datetime', inplace=True)\n",
    "        \n",
    "        if start and end:\n",
    "            obs = obs[date_parser(start):date_parser(end)]\n",
    "        elif start:\n",
    "            obs = obs[date_parser(start):]\n",
    "        elif end:\n",
    "            obs = obs[:date_parser(end)]\n",
    "        \n",
    "        self.data = obs.resample(scale).mean()\n",
    "        \n",
    "        self.mean_square = (self.data * self.data).mean()\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \n",
    "        return self.data[key]\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)\n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        return iter(self.data)\n",
    "\n",
    "    def compare(self, target, simulation, scales=None, desc=None, unit=None, rel=False, figsize=(16,9), dpi=100):\n",
    "\n",
    "        if not scales:\n",
    "            scales = {'Daily': 'D', 'Weekly': 'W', 'Monthly': 'M'}\n",
    "\n",
    "        fig, axes = plt.subplots(ncols=3, \n",
    "                                 nrows=len(scales),\n",
    "                                 figsize=figsize,\n",
    "                                 dpi=dpi,\n",
    "                                 constrained_layout=True)\n",
    "\n",
    "        if desc:\n",
    "            fig.suptitle(desc)\n",
    "\n",
    "        for i, (Tstr, T) in enumerate(scales.items()):\n",
    "            comp_plot, diff_plot, hist_plot = axes[i, :]\n",
    "            \n",
    "            obs_resampled = self[target].resample(T).mean()\n",
    "            sim_resampled = simulation[target].resample(T).mean()\n",
    "\n",
    "            err = obs_resampled - sim_resampled        \n",
    "            if rel:\n",
    "                err = err / obs_resampled.abs()\n",
    "\n",
    "            data = pd.DataFrame({'Observations': obs_resampled, 'Simulation': sim_resampled})\n",
    "            sns.lineplot(data=data, ax=comp_plot)\n",
    "            comp_plot.set_title(Tstr)\n",
    "            comp_plot.set_xlabel('')\n",
    "            if unit:\n",
    "                comp_plot.set_ylabel(f\"[{unit}]\")\n",
    "\n",
    "            sns.lineplot(data=err, ax=diff_plot)\n",
    "            plt.setp(diff_plot.get_xticklabels(), rotation=20)\n",
    "            diff_plot.set_xlabel('')\n",
    "            if rel:\n",
    "                diff_plot.set_ylabel(\"Relative error\")\n",
    "                diff_plot.yaxis.set_major_formatter(mtick.PercentFormatter(xmax=1.0))\n",
    "            elif unit:\n",
    "                diff_plot.set_ylabel(f\"Error [{unit}]\")\n",
    "            else:\n",
    "                diff_plot.set_ylabel(\"Error\")\n",
    "\n",
    "            sns.histplot(y=err, kde=True, stat='probability', ax=hist_plot)\n",
    "            y1, y2 = diff_plot.get_ylim()\n",
    "            hist_plot.set_ylim(y1,y2)\n",
    "            hist_plot.set_yticklabels([])\n",
    "            hist_plot.set_ylabel('')\n",
    "        \n",
    "        return fig\n",
    "    \n",
    "    def metric(self, target, simulation):\n",
    "        \n",
    "        diff = self[target] - simulation[target].resample(self.scale).mean()\n",
    "        \n",
    "        return np.sqrt((diff * diff).mean() / self.mean_square[target])\n",
    "\n",
    "    \n",
    "class monitor:\n",
    "    def __init__(self, interval):\n",
    "        self.datetime = []\n",
    "        self.cpu_usage = []\n",
    "        self.memory_usage = []\n",
    "        self.interval = interval\n",
    "        self.running = False\n",
    "        \n",
    "        self.thread = Thread(target=self.run, args=())\n",
    "        self.thread.daemon = True\n",
    "        self.start()\n",
    "\n",
    "    def sample(self):\n",
    "        self.datetime.append(datetime.now())\n",
    "        self.cpu_usage.append(psutil.cpu_percent())\n",
    "        self.memory_usage.append(psutil.virtual_memory().percent)\n",
    "        \n",
    "    def run(self):\n",
    "        while self.running:\n",
    "            self.sample()\n",
    "            sleep(self.interval)\n",
    "    \n",
    "    def start(self):\n",
    "        self.running = True\n",
    "        self.thread.start()\n",
    "        \n",
    "    def stop(self):\n",
    "        self.running = False\n",
    "        self.thread.join()\n",
    "        \n",
    "    def plot(self, figsize=(16,9), dpi=100):\n",
    "        self.stop()\n",
    "        stats = pd.DataFrame({'CPU': self.cpu_usage, 'Memory': self.memory_usage}, index=self.datetime)\n",
    "        fig = plt.figure(figsize=figsize, dpi=dpi)\n",
    "        axes = fig.add_subplot()\n",
    "        axes.set_title('Resource Monitor')\n",
    "        axes.set_xlabel('Time')\n",
    "        axes.set_ylabel('Usage [%]')\n",
    "        sns.lineplot(data=stats, ax=axes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(address)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GEOtopRunLogVars(GEOtopRun):\n",
    " \n",
    "    def preprocess(self, working_dir, *args, **kwargs):\n",
    "        \n",
    "        for key, value in kwargs.items():\n",
    "            if variables.type[key] == 'log':\n",
    "                kwargs[key] = 10 ** value\n",
    "                \n",
    "        super().preprocess(working_dir, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GEOtopRunLogVars(inputs_path,\n",
    "                         run_args={'check': True, \n",
    "                                   'capture_output': True, \n",
    "                                   'timeout': timeout})\n",
    "\n",
    "variables = pd.read_csv(variables_path, index_col='name')\n",
    "variables['synth'] = [uniform(low=var.lower, high=var.upper) for name, var in variables.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(*args, sim=None, **kwargs):\n",
    "    if sim is None:\n",
    "        with TemporaryDirectory(dir=systmpfs) as tmpdir:\n",
    "            try:\n",
    "                sim = model.eval(tmpdir, *args, **kwargs)\n",
    "            except CalledProcessError:\n",
    "                    return np.nan\n",
    "            except TimeoutExpired:\n",
    "                    return np.nan\n",
    "    return sum(w * synth.metric(t, sim) for w, t in zip(weights, targets)) / sum(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory(dir=systmpfs) as tmpdir:\n",
    "    synth = model.eval(tmpdir, **variables.synth.to_dict())\n",
    "\n",
    "synth = observations(synth, scale=scale, start=startdate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory(dir=systmpfs) as tmpdir:\n",
    "    sim = model.eval(tmpdir)\n",
    "    print(f\"Before optimization loss is {loss_function(sim=sim)}\")\n",
    "    for t in targets:\n",
    "        synth.compare(t, sim, desc=t)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {name: ng.p.Scalar(init=value.suggested, lower=value.lower, upper=value.upper) \n",
    "          for name, value in variables.iterrows()}\n",
    "\n",
    "optimizer = ng.optimizers.registry[algorithm](parametrization=ng.p.Instrumentation(**kwargs),\n",
    "                                              budget=budget,\n",
    "                                              num_workers=num_workers)\n",
    "\n",
    "logfile = NamedTemporaryFile(dir=systmpfs)\n",
    "logger = ng.callbacks.ParametersLogger(logfile.name)\n",
    "optimizer.register_callback(\"tell\",  logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation = optimizer.minimize(loss_function, executor=client, batch_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = pd.DataFrame(logger.load())\n",
    "\n",
    "samples.dropna(subset=['#loss'], inplace=True)\n",
    "points = samples[variables.index].to_numpy()\n",
    "losses = samples['#loss'].to_numpy()\n",
    "\n",
    "problem = {'num_vars': variables.shape[0],\n",
    "           'names': variables.index,\n",
    "           'bounds': list(zip(variables.lower, variables.upper))}\n",
    "\n",
    "SA = delta.analyze(problem, points, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables['best'] = pd.Series(recommendation.kwargs)\n",
    "variables['err'] = 3 * (variables.synth - variables.best).abs() / (variables.upper - variables.lower)\n",
    "\n",
    "pd.concat([variables, SA.to_df()], axis=1).sort_values('err')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = logger.to_hiplot_experiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_columns = ['uid', \n",
    "                  'from_uid', \n",
    "                  '#parametrization', \n",
    "                  '#optimizer', \n",
    "                  '#optimizer#noise_handling', \n",
    "                  '#optimizer#mutation',\n",
    "                  '#optimizer#crossover',\n",
    "                  '#optimizer#initialization',\n",
    "                  '#optimizer#scale',\n",
    "                  '#optimizer#recommendation',\n",
    "                  '#optimizer#F1',\n",
    "                  '#optimizer#F2',\n",
    "                  '#optimizer#popsize',\n",
    "                  '#optimizer#propagate_heritage',\n",
    "                  '#session', \n",
    "                  '#lineage',\n",
    "                  '#meta-sigma']\n",
    "\n",
    "for name in variables.index:\n",
    "    hidden_columns.append(name + '#sigma')\n",
    "    hidden_columns.append(name + '#sigma#sigma')\n",
    "    \n",
    "table = experiment.display_data(hip.Displays.TABLE)\n",
    "table.update({'hide': hidden_columns,\n",
    "              'order_by': [['#num-tell', 'asc']]})\n",
    "\n",
    "plot = experiment.display_data(hip.Displays.PARALLEL_PLOT)\n",
    "plot.update({'hide': [*hidden_columns, '#num-tell'],\n",
    "             'order': ['#generation', *variables.index, '#loss']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "experiment.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with TemporaryDirectory() as tmpdir:\n",
    "    print(f\"After optimization loss is {recommendation.loss}\")\n",
    "    sim = model.eval(tmpdir, **recommendation.kwargs)\n",
    "    for t in targets:\n",
    "        synth.compare(t, sim, desc=t)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables.err.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
